from .policies import DTPolicy, SB3Policy, ObliqueDTPolicy
from .viper import DecisionTreeExtractor
from .feature_utils import mask_features, auto_generate_mask

from stable_baselines3.common.utils import check_for_correct_spaces

import numpy as np
from copy import deepcopy
from tqdm import tqdm
import yaml
import time
from pathlib import Path
from joblib import dump
from operator import itemgetter

# edited from Interpreter repository
class Interpreter(DecisionTreeExtractor):
    """
    A class to interpret a neural net policy using a decision tree policy.
    It follows algorithm 1 from https://arxiv.org/abs/2405.14956
    By default, the trajectories will be sampled in a DAgger-like way.

    Parameters
    ----------
    oracle : object
        The oracle model that generates the data for training.
        Usually a stable-baselines3 model from the hugging face hub.
    learner : object
        The decision tree policy.
    env : object
        The environment in which the policies are evaluated (gym.Env).
    data_per_iter : int, optional
        The number of data points to generate per iteration (default is 5000).
    use_original_obs : bool, optional
        Whether to use original observations for tree training.
    kwargs : optional

    Attributes
    ----------
    oracle : object
        The oracle model that generates the data for training.
    learner : object
        The decision tree policy to be trained.
    data_per_iter : int
        The number of data points to generate per iteration.
    env : object
        The monitored environment in which the policies are evaluated.
    tree_policies : list
        A list to store the trained tree policies over iterations.
    tree_policies_rewards : list
        A list to store the rewards of the trained tree policies over iterations.
    """

    def __init__(self, oracle, learner, env, ff_file, data_per_iter=5000, use_original_obs=False, feature_names = None, **kwargs):
        assert isinstance(oracle, SB3Policy) and (
            isinstance(learner, DTPolicy) or isinstance(learner, ObliqueDTPolicy)
        )
        super().__init__(model=oracle, env=env, data_per_iter=data_per_iter, dtpolicy=None)
        self._learner = learner
        self._policy = deepcopy(learner)
        self.max_tree_reward = float('-inf')
        self._ff_file = ff_file
        self.use_original_obs = use_original_obs
        self._original_feature_names = feature_names
        self._setup_masking()
        self._validate_spaces()
        
        self.list_eval = []  
        self.times = []      
        
        self.best_dt = None

    def _setup_masking(self):
        self._mask_indices = None
        
        try:
            if self._ff_file:
                with open(self._ff_file, 'r') as f:
                    config = yaml.safe_load(f)
                self._mask_indices = config.get('FEATURE_MASK', {}).get('keep_indices', None)
        except (FileNotFoundError, yaml.YAMLError):
            print(f"Failed to load feature mask configuration from {self._ff_file}, will generate mask automatically")
        
        # if no valid mask_indices is found, auto generate the mask
        if self._mask_indices is None:
            try:
                feature_descriptions = self._original_feature_names
                if feature_descriptions is None:
                    raise AttributeError("No feature descriptions available")
                self._mask_indices = auto_generate_mask(feature_descriptions)
            except (AttributeError, TypeError) as e:
                print(f"Warning: Failed to generate feature mask: {e}")
                return
            
    def _validate_spaces(self):
        """validate the space compatibility between the environment and the policy"""
        check_for_correct_spaces(
            self.env,
            self._policy.observation_space,
            self._policy.action_space,
        )
        check_for_correct_spaces(
            self.env, self.model.observation_space, self.model.action_space
        )

    def _get_observation(self, env_obs):
        """get the appropriate observation"""
        if self.use_original_obs:
            obs = self.env.get_original_obs()
        else:
            obs = env_obs
        return mask_features(obs, self._mask_indices)

    def fit(self, nb_timesteps):
        """
        Train the decision tree policy using data generated by the oracle.

        Parameters
        ----------
        nb_timesteps : int
            The number of environment transitions used for learning.
        """
        start_time = time.time()
        nb_iter = int(max(1, nb_timesteps // self.data_per_iter))
        print("Collecting data...")
        S, A, S_masked = self.collect_data()
        print("Fitting tree nb {} ...".format(0))        
        self._learner.fit(S_masked, A)
        self._policy = deepcopy(self._learner)
        S1, S_masked_new, tree_reward = self.collect_data_dt(self._policy, self.data_per_iter)
        self.times.append(time.time() - start_time)
        
        print("Tree {} Evaluation: {}".format(0, tree_reward))
        self.list_eval.append(tree_reward)
        
        # initialize best tree
        self.max_tree_reward = tree_reward
        self.best_dt = deepcopy(self._learner)
        print(f"New best tree reward: {tree_reward}")
        
        S = np.concatenate((S, S1))
        A = np.concatenate((A, self.model.predict(S1)[0]))
        S_masked = np.concatenate((S_masked, S_masked_new))

        for t in range(1, nb_iter):
            print("Fitting tree nb {} ...".format(t))
            self._learner.fit(S_masked, A)
            S_tree, S_masked_new, tree_reward = self.collect_data_dt(self._learner, self.data_per_iter)
            self.times.append(time.time() - start_time)
            
            print("Tree {} Evaluation: {}".format(t, tree_reward))
            self.list_eval.append(tree_reward)
            
            if tree_reward > self.max_tree_reward:
                self.max_tree_reward = tree_reward
                self._policy = deepcopy(self._learner)
                self.best_dt = deepcopy(self._learner)
                print("New best tree reward: {}".format(tree_reward))

            S = np.concatenate((S, S_tree))
            A = np.concatenate((A, self.model.predict(S_tree)[0]))
            S_masked = np.concatenate((S_masked, S_masked_new))
            
        print(f"Training completed, best tree reward: {self.max_tree_reward}")

    def policy(self, obs):
        return self._policy.predict(obs)
    
    def collect_data(self):
        S, A, S_masked = [], [], []
        s = self.env.reset()
        s_masked = self._get_observation(s)
        for i in tqdm(range(self.data_per_iter)):
            action = self.model.predict(s, deterministic=True)[0]
            S.append(s[0]) #unvec
            S_masked.append(s_masked[0]) #unvec
            A.append(action[0]) #unvec
            s, _, done, _ = self.env.step(action)
            s_masked = self._get_observation(s)
            if done:
                s = self.env.reset()
                s_masked = self._get_observation(s)
        return np.array(S), np.array(A), np.array(S_masked)
    
    def collect_data_dt(self, policy, nb_data):
        """
        Generate data by running the policy in the environment.

        Parameters
        ----------
        policy:
            The policy to generate transitions
        nb_data : int
            The number of data points to generate.

        Returns
        -------
        S : np.ndarray
            Array of collected observations
        S_masked : np.ndarray
            Array of masked observations
        mean_reward : float
            Mean episode reward achieved by the policy
        """
        episodes = []
        ep_reward = 0
        S = np.zeros((nb_data, self.env.observation_space.shape[0]))
        s = self.env.reset()
        s_masked = self._get_observation(s)
        S_masked = np.zeros((nb_data, s_masked[0].shape[0])) 

        for i in tqdm(range(nb_data)):
            action, _ = policy.predict(s_masked)
            S[i] = s
            S_masked[i] = s_masked[0]  # unvec
            s, r, done, infos = self.env.step(action)
            s_masked = self._get_observation(s)
            ep_reward += r
            if done:
                s = self.env.reset()
                s_masked = self._get_observation(s)
                episodes.append(ep_reward)
                ep_reward = 0
        if len(episodes) < 1:
            episodes.append(ep_reward)
        return S, S_masked, np.mean(episodes)
    
    def save_best_tree(self, out_path):
        """
        Only save the best decision tree to the specified path
        
        Parameters
        ----------
        out_path : Path
            Output path
        """
        out_path.mkdir(parents=True, exist_ok=True)
        
        # save the best tree found during training
        best_fpath = f"best_tree_reward-{self.max_tree_reward:.2f}.interpreter"
        dump(self.best_dt, out_path / best_fpath)
        
        # save the evaluation results of the best tree
        eval_info_path = out_path / "best_tree_info.txt"
        with open(eval_info_path, "w") as f:
            f.write(f"Best tree reward: {self.max_tree_reward}\n")
            f.write(f"Training iterations: {len(self.list_eval)}\n")
            f.write(f"Training time: {self.times[-1]:.2f} seconds\n")
