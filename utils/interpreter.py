from .policies import DTPolicy, SB3Policy, ObliqueDTPolicy
from .viper import DecisionTreeExtractor

from stable_baselines3.common.utils import check_for_correct_spaces

import numpy as np
from copy import deepcopy
from tqdm import tqdm

# edited from Interpreter repository
class Interpreter(DecisionTreeExtractor):
    """
    A class to interpret a neural net policy using a decision tree policy.
    It follows algorithm 1 from https://arxiv.org/abs/2405.14956
    By default, the trajectories will be sampled in a DAgger-like way.

    Parameters
    ----------
    oracle : object
        The oracle model that generates the data for training.
        Usually a stable-baselines3 model from the hugging face hub.
    learner : object
        The decision tree policy.
    env : object
        The environment in which the policies are evaluated (gym.Env).
    data_per_iter : int, optional
        The number of data points to generate per iteration (default is 5000).
    kwargs : optional

    Attributes
    ----------
    oracle : object
        The oracle model that generates the data for training.
    learner : object
        The decision tree policy to be trained.
    data_per_iter : int
        The number of data points to generate per iteration.
    env : object
        The monitored environment in which the policies are evaluated.
    tree_policies : list
        A list to store the trained tree policies over iterations.
    tree_policies_rewards : list
        A list to store the rewards of the trained tree policies over iterations.
    """

    def __init__(self, oracle, learner, env, data_per_iter=5000, **kwargs):
        assert isinstance(oracle, SB3Policy) and (
            isinstance(learner, DTPolicy) or isinstance(learner, ObliqueDTPolicy)
        )
        super().__init__(model=oracle, env=env, data_per_iter=data_per_iter, dtpolicy=None)
        self._learner = learner
        self._policy = deepcopy(learner)
        self.max_tree_reward = float('-inf')

        check_for_correct_spaces(
            self.env,
            self._policy.observation_space,
            self._policy.action_space,
        )
        check_for_correct_spaces(
            self.env, self.model.observation_space, self.model.action_space
        )

    def fit(self, nb_timesteps):
        """
        Train the decision tree policy using data generated by the oracle.

        Parameters
        ----------
        nb_timesteps : int
            The number of environment transitions used for learning.
        """
        print("Fitting tree nb {} ...".format(0))
        nb_iter = int(max(1, nb_timesteps // self.data_per_iter))
        S, A = self.collect_data()
        self._learner.fit(S, A)
        self._policy = deepcopy(self._learner)
        S1, tree_reward = self.collect_data_dt(self._policy, self.data_per_iter)
        print("Tree reward: {}".format(tree_reward))
        current_max_reward = tree_reward
        S = np.concatenate((S, S1))
        A = np.concatenate((A, self.model.predict(S1)[0]))

        for t in range(1, nb_iter):
            print("Fitting tree nb {} ...".format(t + 1))
            self._learner.fit(S, A)
            S_tree, tree_reward = self.collect_data_dt(self._learner, self.data_per_iter) 
            if tree_reward > current_max_reward:
                current_max_reward = tree_reward
                self._policy = deepcopy(self._learner)
                print("New best tree reward: {}".format(tree_reward))

            S = np.concatenate((S, S_tree))
            A = np.concatenate((A, self.model.predict(S_tree)[0]))
        self.max_tree_reward = current_max_reward

    def policy(self, obs):
        return self._policy.predict(obs)

    def collect_data_dt(self, policy, nb_data):
        """
        Generate data by running the policy in the environment.

        Parameters
        ----------
        policy:
            The policy to generate transitions
        nb_data : int
            The number of data points to generate.

        Returns
        -------
        S : np.ndarray
            Array of collected observations
        mean_reward : float
            Mean episode reward achieved by the policy
        """
        episodes = []
        ep_reward = 0
        S = np.zeros((nb_data, self.env.observation_space.shape[0]))
        s = self.env.reset()
        for i in tqdm(range(nb_data)):
            action, _ = policy.predict(s)
            S[i] = s
            s, r, done, infos = self.env.step(action)
            ep_reward += r
            if done:
                s = self.env.reset()
                episodes.append(ep_reward)
                ep_reward = 0
        if len(episodes) < 1:
            episodes.append(ep_reward)
        return S, np.mean(episodes)
